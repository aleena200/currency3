{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd0796-d59f-4a80-b498-0f0b2d287d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Found 450 images belonging to 6 classes.\n",
      "Found 120 images belonging to 6 classes.\n",
      "Training VGG16 Model... 🚀\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 8s/step - accuracy: 0.1907 - loss: 1.9054 - val_accuracy: 0.3333 - val_loss: 1.7385\n",
      "Epoch 2/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 7s/step - accuracy: 0.2454 - loss: 1.8445 - val_accuracy: 0.3500 - val_loss: 1.6622\n",
      "Epoch 3/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 7s/step - accuracy: 0.2542 - loss: 1.6876 - val_accuracy: 0.3667 - val_loss: 1.6100\n",
      "Epoch 4/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 7s/step - accuracy: 0.2961 - loss: 1.6216 - val_accuracy: 0.4167 - val_loss: 1.5634\n",
      "Epoch 5/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 7s/step - accuracy: 0.3813 - loss: 1.5627 - val_accuracy: 0.5250 - val_loss: 1.5104\n",
      "Epoch 6/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 8s/step - accuracy: 0.4088 - loss: 1.5010 - val_accuracy: 0.5667 - val_loss: 1.4344\n",
      "Epoch 7/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 8s/step - accuracy: 0.4642 - loss: 1.4510 - val_accuracy: 0.6000 - val_loss: 1.3976\n",
      "Epoch 8/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 7s/step - accuracy: 0.5007 - loss: 1.4070 - val_accuracy: 0.6000 - val_loss: 1.3533\n",
      "Epoch 9/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 7s/step - accuracy: 0.4920 - loss: 1.3616 - val_accuracy: 0.6083 - val_loss: 1.2964\n",
      "Epoch 10/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 7s/step - accuracy: 0.5588 - loss: 1.2874 - val_accuracy: 0.5917 - val_loss: 1.2842\n",
      "Epoch 11/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 7s/step - accuracy: 0.5325 - loss: 1.2801 - val_accuracy: 0.6500 - val_loss: 1.2225\n",
      "Epoch 12/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 7s/step - accuracy: 0.6233 - loss: 1.1803 - val_accuracy: 0.6083 - val_loss: 1.2002\n",
      "Epoch 13/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 7s/step - accuracy: 0.6205 - loss: 1.2111 - val_accuracy: 0.6500 - val_loss: 1.1490\n",
      "Epoch 14/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 7s/step - accuracy: 0.6346 - loss: 1.1037 - val_accuracy: 0.6833 - val_loss: 1.1046\n",
      "Epoch 15/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 7s/step - accuracy: 0.6348 - loss: 1.1068 - val_accuracy: 0.7083 - val_loss: 1.0696\n",
      "Epoch 16/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 7s/step - accuracy: 0.6972 - loss: 1.0101 - val_accuracy: 0.7333 - val_loss: 1.0284\n",
      "Epoch 17/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 7s/step - accuracy: 0.7029 - loss: 1.0022 - val_accuracy: 0.7333 - val_loss: 0.9926\n",
      "Epoch 18/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 7s/step - accuracy: 0.6847 - loss: 1.0277 - val_accuracy: 0.6833 - val_loss: 0.9839\n",
      "Epoch 19/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 8s/step - accuracy: 0.6853 - loss: 1.0140 - val_accuracy: 0.7500 - val_loss: 0.9422\n",
      "Epoch 20/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 7s/step - accuracy: 0.7589 - loss: 0.8863 - val_accuracy: 0.7583 - val_loss: 0.8926\n",
      "Epoch 21/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 7s/step - accuracy: 0.7538 - loss: 0.8432 - val_accuracy: 0.7667 - val_loss: 0.8615\n",
      "Epoch 22/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 7s/step - accuracy: 0.7024 - loss: 0.8609 - val_accuracy: 0.7500 - val_loss: 0.8571\n",
      "Epoch 23/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 7s/step - accuracy: 0.7438 - loss: 0.8239 - val_accuracy: 0.8250 - val_loss: 0.8271\n",
      "Epoch 24/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 7s/step - accuracy: 0.7604 - loss: 0.8415 - val_accuracy: 0.8167 - val_loss: 0.7856\n",
      "Epoch 25/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 7s/step - accuracy: 0.7993 - loss: 0.7393 - val_accuracy: 0.7500 - val_loss: 0.7880\n",
      "Epoch 26/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 7s/step - accuracy: 0.7893 - loss: 0.7329 - val_accuracy: 0.8000 - val_loss: 0.7670\n",
      "Epoch 27/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 7s/step - accuracy: 0.7892 - loss: 0.7413 - val_accuracy: 0.7750 - val_loss: 0.7681\n",
      "Epoch 28/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 7s/step - accuracy: 0.7820 - loss: 0.7660 - val_accuracy: 0.8500 - val_loss: 0.7332\n",
      "Epoch 29/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 7s/step - accuracy: 0.7877 - loss: 0.6897 - val_accuracy: 0.7833 - val_loss: 0.7056\n",
      "Epoch 30/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 7s/step - accuracy: 0.8365 - loss: 0.6652 - val_accuracy: 0.8250 - val_loss: 0.7214\n"
=======
      "Found 2049 images belonging to 6 classes.\n",
      "Found 546 images belonging to 6 classes.\n",
      "Training VGG16 Model... 🚀\n",
      "Epoch 1/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 4s/step - accuracy: 0.1893 - loss: 1.8918 - val_accuracy: 0.3516 - val_loss: 1.6596\n",
      "Epoch 2/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 4s/step - accuracy: 0.3138 - loss: 1.6517 - val_accuracy: 0.4689 - val_loss: 1.5363\n",
      "Epoch 3/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m249s\u001b[0m 4s/step - accuracy: 0.3913 - loss: 1.5520 - val_accuracy: 0.5018 - val_loss: 1.4324\n",
      "Epoch 4/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 4s/step - accuracy: 0.4150 - loss: 1.4666 - val_accuracy: 0.5055 - val_loss: 1.3873\n",
      "Epoch 5/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 4s/step - accuracy: 0.4406 - loss: 1.4222 - val_accuracy: 0.5293 - val_loss: 1.3027\n",
      "Epoch 6/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 4s/step - accuracy: 0.4815 - loss: 1.3393 - val_accuracy: 0.5623 - val_loss: 1.2567\n",
      "Epoch 7/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 4s/step - accuracy: 0.4972 - loss: 1.3056 - val_accuracy: 0.5769 - val_loss: 1.1939\n",
      "Epoch 8/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 4s/step - accuracy: 0.4899 - loss: 1.3100 - val_accuracy: 0.5897 - val_loss: 1.1751\n",
      "Epoch 9/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 4s/step - accuracy: 0.5367 - loss: 1.2433 - val_accuracy: 0.5879 - val_loss: 1.1442\n",
      "Epoch 10/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 4s/step - accuracy: 0.5683 - loss: 1.1367 - val_accuracy: 0.5916 - val_loss: 1.1139\n",
      "Epoch 11/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 4s/step - accuracy: 0.5604 - loss: 1.1868 - val_accuracy: 0.6209 - val_loss: 1.0758\n",
      "Epoch 12/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 4s/step - accuracy: 0.5967 - loss: 1.1059 - val_accuracy: 0.6245 - val_loss: 1.0639\n",
      "Epoch 13/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 4s/step - accuracy: 0.6242 - loss: 1.0439 - val_accuracy: 0.6410 - val_loss: 1.0616\n",
      "Epoch 14/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 4s/step - accuracy: 0.6005 - loss: 1.0382 - val_accuracy: 0.6007 - val_loss: 1.0372\n",
      "Epoch 15/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 4s/step - accuracy: 0.6146 - loss: 1.0137 - val_accuracy: 0.6630 - val_loss: 0.9794\n",
      "Epoch 16/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 4s/step - accuracy: 0.6369 - loss: 0.9704 - val_accuracy: 0.6319 - val_loss: 0.9849\n",
      "Epoch 17/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 4s/step - accuracy: 0.6711 - loss: 0.9464 - val_accuracy: 0.6484 - val_loss: 0.9594\n",
      "Epoch 18/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 4s/step - accuracy: 0.6299 - loss: 0.9678 - val_accuracy: 0.6392 - val_loss: 0.9570\n",
      "Epoch 19/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 3s/step - accuracy: 0.6773 - loss: 0.8776 - val_accuracy: 0.6612 - val_loss: 0.9306\n",
      "Epoch 20/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 4s/step - accuracy: 0.6727 - loss: 0.8894 - val_accuracy: 0.6612 - val_loss: 0.9242\n",
      "Epoch 21/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 4s/step - accuracy: 0.6704 - loss: 0.9012 - val_accuracy: 0.6667 - val_loss: 0.9095\n",
      "Epoch 22/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 4s/step - accuracy: 0.6631 - loss: 0.8747 - val_accuracy: 0.6813 - val_loss: 0.9021\n",
      "Epoch 23/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 4s/step - accuracy: 0.6901 - loss: 0.8503 - val_accuracy: 0.6905 - val_loss: 0.8995\n",
      "Epoch 24/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 4s/step - accuracy: 0.6966 - loss: 0.8540 - val_accuracy: 0.6685 - val_loss: 0.8842\n",
      "Epoch 25/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 3s/step - accuracy: 0.7020 - loss: 0.8123 - val_accuracy: 0.6868 - val_loss: 0.8781\n",
      "Epoch 26/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 4s/step - accuracy: 0.7020 - loss: 0.8262 - val_accuracy: 0.6868 - val_loss: 0.8788\n",
      "Epoch 27/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 4s/step - accuracy: 0.7059 - loss: 0.7978 - val_accuracy: 0.7015 - val_loss: 0.8546\n",
      "Epoch 28/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 4s/step - accuracy: 0.6961 - loss: 0.8261 - val_accuracy: 0.6960 - val_loss: 0.8653\n",
      "Epoch 29/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 4s/step - accuracy: 0.7185 - loss: 0.8248 - val_accuracy: 0.6905 - val_loss: 0.8331\n",
      "Epoch 30/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 4s/step - accuracy: 0.6986 - loss: 0.7917 - val_accuracy: 0.6905 - val_loss: 0.8360\n"
>>>>>>> caee60a03d5f3a245979498f8a668b9fb2206b63
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ VGG16 Model saved as 'vgg16_model.h5'.\n",
      "✅ VGG16 Model Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from tkinter import messagebox\n",
    "from PIL import Image, ImageTk\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import threading  # Used for running classification in the background\n",
    "\n",
    "# --------------------------- #\n",
    "# 🔹 Define Paths & Parameters\n",
    "# --------------------------- #\n",
    "train_dir = \"output/train\"\n",
    "val_dir = \"output/val\"\n",
    "input_shape = (224, 224, 3)\n",
    "batch_size = 32\n",
    "NUM_CLASSES = 6 # Updated number of traffic sign classes\n",
    "\n",
    "# --------------------------- #\n",
    "# 🔹 Data Preparation (for VGG16)\n",
    "# --------------------------- #\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "val_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "train_data = train_datagen.flow_from_directory(train_dir, target_size=(224, 224), \n",
    "                                               batch_size=batch_size, class_mode='categorical')\n",
    "val_data = val_datagen.flow_from_directory(val_dir, target_size=(224, 224), \n",
    "                                           batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "# --------------------------- #\n",
    "# 🔹 Function to Create VGG16 Model\n",
    "# --------------------------- #\n",
    "def create_vgg16_model():\n",
    "    base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # Freeze pre-trained layers\n",
    "\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(NUM_CLASSES, activation=\"softmax\")(x)  # Ensure correct number of classes\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    return model\n",
    "\n",
    "# --------------------------- #\n",
    "# 🔹 Train and Save VGG16 Model (if not exists)\n",
    "# --------------------------- #\n",
    "vgg16_model_path = \"vgg16_model.h5\"\n",
    "\n",
    "if not os.path.exists(vgg16_model_path):\n",
    "    print(\"Training VGG16 Model... 🚀\")\n",
    "    vgg16_model = create_vgg16_model()\n",
    "    vgg16_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                        loss=\"categorical_crossentropy\",\n",
    "                        metrics=[\"accuracy\"])\n",
    "    \n",
    "    vgg16_model.fit(train_data, epochs=30, validation_data=val_data)\n",
    "    vgg16_model.save(vgg16_model_path)\n",
    "    print(f\"✅ VGG16 Model saved as '{vgg16_model_path}'.\")\n",
    "else:\n",
    "    print(f\"✅ Found existing model '{vgg16_model_path}', skipping training.\")\n",
    "\n",
    "# Load the trained model\n",
    "vgg16_model = create_vgg16_model()\n",
    "vgg16_model.load_weights(vgg16_model_path)\n",
    "print(\"✅ VGG16 Model Loaded Successfully!\")\n",
    "\n",
    "# --------------------------- #\n",
    "# 🔹 Function: Preprocess Image for VGG16\n",
    "# --------------------------- #\n",
    "def preprocess_for_vgg16(image):\n",
    "    image = image.resize((224, 224))  # Resize image\n",
    "    image = np.array(image) / 255.0  # Normalize\n",
    "    image = np.expand_dims(image, axis=0)  # Expand dims for model\n",
    "    return image\n",
    "\n",
    "# --------------------------- #\n",
    "# 🔹 Function: Classify Image Using VGG16\n",
    "# --------------------------- #\n",
    "def classify_image(image_path):\n",
    "    image = Image.open(image_path)  # Open image\n",
    "    processed_img = preprocess_for_vgg16(image)  # Preprocess for VGG16\n",
    "    predictions = vgg16_model.predict(processed_img)  # Predict\n",
    "\n",
    "    class_id = np.argmax(predictions)\n",
    "    confidence = predictions[0][class_id]\n",
    "    \n",
    "    # Class labels\n",
    "    class_labels = ['10_new', '20_new', '50_new', '100_new', '200_new','500_new']\n",
    "    label = f\"{class_labels[class_id]} ({confidence:.2f})\"\n",
    "    \n",
    "    return label\n",
    "\n",
    "# --------------------------- #\n",
    "# 🔹 Function to Open File Dialog and Select Image\n",
    "# --------------------------- #\n",
    "def select_image():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the root window\n",
    "    image_path = filedialog.askopenfilename(title=\"Select Image\", filetypes=[(\"Image files\", \"*.jpg;*.jpeg;*.png\")])\n",
    "    return image_path\n",
    "\n",
    "# --------------------------- #\n",
    "# 🔹 Function to Display the Classification Result\n",
    "# --------------------------- #\n",
    "def show_classification_result(image_path):\n",
    "    result_label.config(text=\"Processing... Please wait.\", fg=\"black\")\n",
    "    \n",
    "    # Run classification in a background thread to avoid blocking UI\n",
    "    threading.Thread(target=run_classification, args=(image_path,)).start()\n",
    "\n",
    "# --------------------------- #\n",
    "# 🔹 Function to Run Classification in the Background\n",
    "# --------------------------- #\n",
    "def run_classification(image_path):\n",
    "    try:\n",
    "        # Classify the uploaded image\n",
    "        result = classify_image(image_path)\n",
    "        \n",
    "        # Update the result label in the main thread\n",
    "        result_label.config(text=f\"Prediction: {result}\", fg=\"green\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        # In case of error, display the error message\n",
    "        result_label.config(text=f\"Error: {str(e)}\", fg=\"red\")\n",
    "\n",
    "# --------------------------- #\n",
    "# 🔹 Function to Load and Display Image in Tkinter\n",
    "# --------------------------- #\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image.thumbnail((400, 400))  # Resize for better display\n",
    "    img_tk = ImageTk.PhotoImage(image)\n",
    "    \n",
    "    # Keep a reference of the image to prevent it from being garbage collected\n",
    "    preview_label.image = img_tk  \n",
    "    preview_label.config(image=img_tk)\n",
    "\n",
    "# --------------------------- #\n",
    "# 🔹 Create the Tkinter GUI\n",
    "# --------------------------- #\n",
    "window = tk.Tk()\n",
    "window.title(\"Image Classification\")\n",
    "window.geometry(\"600x600\")\n",
    "\n",
    "# Create an upload button\n",
    "upload_button = tk.Button(window, text=\"Upload Image\", font=(\"Helvetica\", 14), command=lambda: on_upload())\n",
    "upload_button.pack(pady=10)\n",
    "\n",
    "# Label to display the selected image preview\n",
    "preview_label = tk.Label(window)\n",
    "preview_label.pack(pady=20)\n",
    "\n",
    "# Label for the classification result\n",
    "result_label = tk.Label(window, text=\"Prediction: None\", font=(\"Helvetica\", 14), fg=\"blue\")\n",
    "result_label.pack(pady=20)\n",
    "\n",
    "# --------------------------- #\n",
    "# 🔹 Upload Button Logic\n",
    "# --------------------------- #\n",
    "def on_upload():\n",
    "    image_path = select_image()\n",
    "    if image_path:\n",
    "        load_image(image_path)\n",
    "        show_classification_result(image_path)\n",
    "    else:\n",
    "        classes=['10_new', '20_new', '50_new', '100_new', '200_new','500_new'] \n",
    "        messagebox.showerror(\"Error\", \"No image selected. Please upload an image.\")\n",
    "\n",
    "# Run the Tkinter event loop\n",
    "window.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "3aaed65f-3662-4e83-8f8d-41f9adec8dfc",
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 1,
   "id": "3aaed65f-3662-4e83-8f8d-41f9adec8dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 16:10:55.959851: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-18 16:10:56.719359: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 21.5M/21.5M [00:15<00:00, 1.50MB/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape mismatch in layer #14 (named dense_1)for weight dense_1/kernel. Weight expects shape (256, 5). Received saved weight with shape (256, 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m     30\u001b[0m vgg16_model \u001b[38;5;241m=\u001b[39m create_vgg16_model()\n\u001b[0;32m---> 31\u001b[0m vgg16_model\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvgg16_model.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Class labels for currency notes\u001b[39;00m\n\u001b[1;32m     34\u001b[0m class_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10_new\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m20_new\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m50_new\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100_new\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m200_new\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m500_new\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/legacy/saving/legacy_h5_format.py:440\u001b[0m, in \u001b[0;36m_set_weights\u001b[0;34m(instance, symbolic_weights, weight_values, name, skip_mismatch)\u001b[0m\n\u001b[1;32m    430\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    431\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping loading weights for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdue to mismatch in shape for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    437\u001b[0m                 stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    438\u001b[0m             )\n\u001b[1;32m    439\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 440\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    441\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape mismatch in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    442\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor weight \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msymbolic_weights[i]\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    443\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeight expects shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    444\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived saved weight \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    445\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreceived_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m         )\n\u001b[1;32m    447\u001b[0m     symbolic_weights[i]\u001b[38;5;241m.\u001b[39massign(weight_value)\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(instance, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinalize_state\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m symbolic_weights:\n",
      "\u001b[0;31mValueError\u001b[0m: Shape mismatch in layer #14 (named dense_1)for weight dense_1/kernel. Weight expects shape (256, 5). Received saved weight with shape (256, 6)"
     ]
    }
   ],
>>>>>>> caee60a03d5f3a245979498f8a668b9fb2206b63
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "yolo_model = YOLO(\"yolov8s.pt\")\n",
    "# Configure model to only detect currency notes\n",
    "yolo_model.classes = [67]  # Class ID for currency notes\n",
    "\n",
    "# --------------------------- #\n",
    "# 🔹 Load VGG16 Model for Currency Classification\n",
    "# --------------------------- #\n",
    "def create_vgg16_model():\n",
    "    input_shape = (224, 224, 3)\n",
    "    base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False\n",
    "\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(5, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    return model\n",
    "\n",
    "vgg16_model = create_vgg16_model()\n",
    "vgg16_model.load_weights(\"vgg16_model.h5\")\n",
    "\n",
    "# Class labels for currency notes\n",
    "class_labels = ['10_new', '20_new', '50_new', '100_new', '200_new','500_new']\n",
    "\n",
    "def preprocess_for_vgg16(cropped_img):\n",
    "    if cropped_img.size == 0:\n",
    "        return None\n",
    "    try:\n",
    "        cropped_img = cv2.resize(cropped_img, (224, 224))\n",
    "        cropped_img = cropped_img.astype(\"float32\") / 255.0\n",
    "        cropped_img = np.expand_dims(cropped_img, axis=0)\n",
    "        return cropped_img\n",
    "    except Exception as e:\n",
    "        print(f\"Error in preprocessing: {e}\")\n",
    "        return None\n",
    "\n",
    "def is_valid_currency_note(box, conf, min_area=5000):\n",
    "    # Get box dimensions\n",
    "    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "    area = (x2 - x1) * (y2 - y1)\n",
    "    aspect_ratio = (x2 - x1) / (y2 - y1)\n",
    "    \n",
    "    # Check if the note meets our criteria\n",
    "    return (conf > 0.6 and  # High confidence\n",
    "            area >= min_area and  # Minimum size\n",
    "            1.5 <= aspect_ratio <= 3.0)  # Typical currency note aspect ratio\n",
    "\n",
    "def real_time_detection_and_classification():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    # Set camera properties for better performance\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "\n",
    "    print(\"Starting detection. Press 'q' to quit.\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Create a copy for display\n",
    "        display_frame = frame.copy()\n",
    "\n",
    "        # Run YOLO detection\n",
    "        results = yolo_model(frame, conf=0.5)\n",
    "\n",
    "        best_note = None\n",
    "        highest_conf = 0\n",
    "\n",
    "        for result in results:\n",
    "            boxes = result.boxes\n",
    "            for box in boxes:\n",
    "                conf = float(box.conf[0])\n",
    "                \n",
    "                # Check if this is a valid currency note with higher confidence\n",
    "                if is_valid_currency_note(box, conf):\n",
    "                    if conf > highest_conf:\n",
    "                        highest_conf = conf\n",
    "                        best_note = box\n",
    "\n",
    "        # Process only the best detected note\n",
    "        if best_note is not None:\n",
    "            x1, y1, x2, y2 = map(int, best_note.xyxy[0])\n",
    "            cropped_obj = frame[y1:y2, x1:x2]\n",
    "            processed_img = preprocess_for_vgg16(cropped_obj)\n",
    "\n",
    "            if processed_img is not None:\n",
    "                try:\n",
    "                    predictions = vgg16_model.predict(processed_img, verbose=0)\n",
    "                    note_class = np.argmax(predictions)\n",
    "                    note_conf = predictions[0][note_class]\n",
    "\n",
    "                    if note_conf > 0.7:  # High confidence threshold for currency\n",
    "                        label = f\"{class_labels[note_class]} ({note_conf:.2f})\"\n",
    "                        cv2.rectangle(display_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                        cv2.putText(display_frame, label, (x1, y1 - 10),\n",
    "                                  cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in classification: {e}\")\n",
    "\n",
    "        # Add FPS counter\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        cv2.putText(display_frame, f\"FPS: {int(fps)}\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow(\"Currency Detection\", display_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    real_time_detection_and_classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04a48c4-9c87-4d81-a7d9-f17f3cf997de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
